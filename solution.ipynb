{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66590a06",
   "metadata": {},
   "source": [
    "### 1. Модель на основе LSTM\n",
    "Для оценки работоспособности кода в папке `data` находится датасет, усечённый до 50000 строк.   \n",
    "Для оценки работы предварительно обученной модели, можно перейти в блок 8 и далее.  \n",
    "\n",
    "Модель обучалась на 12 эпохах.  \n",
    "Гиперпараметры модели:  \n",
    "```\n",
    "EMBEDDING_DIM = 128  \n",
    "HIDDEN_DIM = 512  \n",
    "NUM_LAYERS = 1  \n",
    "```\n",
    "Размер батча - 256  \n",
    "  \n",
    "Epoch 1 | Train Loss: 5.439 | Val Loss: 5.311 | Val PPL: 202.47 | Val Accuracy: 17.53%  \n",
    "Epoch 2 | Train Loss: 5.327 | Val Loss: 5.291 | Val PPL: 198.52 | Val Accuracy: 17.64%  \n",
    "Epoch 3 | Train Loss: 5.314 | Val Loss: 5.285 | Val PPL: 197.29 | Val Accuracy: 17.71%  \n",
    "Epoch 4 | Train Loss: 5.309 | Val Loss: 5.281 | Val PPL: 196.49 | Val Accuracy: 17.73%  \n",
    "Epoch 5 | Train Loss: 5.305 | Val Loss: 5.281 | Val PPL: 196.63 | Val Accuracy: 17.75%  \n",
    "Epoch 6 | Train Loss: 5.303 | Val Loss: 5.279 | Val PPL: 196.10 | Val Accuracy: 17.75%  \n",
    "Epoch 7 | Train Loss: 5.302 | Val Loss: 5.277 | Val PPL: 195.72 | Val Accuracy: 17.79%  \n",
    "Epoch 8 | Train Loss: 5.300 | Val Loss: 5.277 | Val PPL: 195.71 | Val Accuracy: 17.76%  \n",
    "Epoch 9 | Train Loss: 5.300 | Val Loss: 5.277 | Val PPL: 195.74 | Val Accuracy: 17.74%  \n",
    "Epoch 10 | Train Loss: 5.299 | Val Loss: 5.275 | Val PPL: 195.48 | Val Accuracy: 17.80%  \n",
    "Epoch 11 | Train Loss: 5.298 | Val Loss: 5.274 | Val PPL: 195.29 | Val Accuracy: 17.75%  \n",
    "Epoch 12 | Train Loss: 5.298 | Val Loss: 5.274 | Val PPL: 195.15 | Val Accuracy: 17.78%  \n",
    "\n",
    "Метрика ROUGE на протяжении всего обучения менялась от 0.1333 до 0.2  \n",
    "Для ускорения расчёта ROUGE, замер происходил на ограниченном количестве \n",
    "ROUGE-1: 0.2000  \n",
    "ROUGE-L: 0.2000  \n",
    "ROUGE-Lsum: 0.2000  \n",
    "\n",
    "Несмотря на большие значения loss, наблюдается её падение. Также наблюдается небольшая разница между train loss и val loss, что говорит об остутствии переобучения.  \n",
    "\n",
    "Было принято решение так же оценить метрику PPL (перплексия), которая показывает неопределённость модели в предсказании следующего токена в последовательности.  \n",
    "Чем меньше значение PPL, тем меньше неопределённости.  \n",
    "\n",
    "Несмотря на невысокие показатели метрик, модель генерирует связный текст.  \n",
    "\n",
    "\n",
    "Пример генерации (автодополнения):  \n",
    "Исходный: `cant wait to get a car walking in`  \n",
    "Дополненный: `cant wait to get a car walking in with the family and i have to go to work`  \n",
    "Кроме детерминированного предсказания, было реализовано предсказание с сэмплированием.  \n",
    "Пример результата:  \n",
    "Исходный: `cant wait to get a car walking in`  \n",
    "Дополненный: `cant wait to get a car walking in with that in my house that has work on saturday`  \n",
    "\n",
    "\n",
    "![График Loss и Accuracy](loss_and_accuracy.png)  \n",
    "![График PPL и Accuracy](ppl_and_accuracy.png)\n",
    "\n",
    "### 2. Модель GPT2 (distilgpt2)\n",
    "Модель хорошо справляется с генерацией небольших текстов, о чём свидетельствуют неплохие, по сравнению с LSTM, метрики ROUGE.  \n",
    "\n",
    "ROUGE-1: 0.5697  \n",
    "ROUGE-2: 0.5048  \n",
    "ROUGE-L: 0.5697  \n",
    "ROUGE-Lsum: 0.5697  \n",
    "  \n",
    "Пример генерации:  \n",
    "Промпт: `just got done watching august`  \n",
    "Референс: `just got done watching august rush love it sooo much`  \n",
    "Генерация: `just got done watching august.com for a few hours.`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0af36",
   "metadata": {},
   "source": [
    "======================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82591010",
   "metadata": {},
   "source": [
    "1. Импорт библиотек, классов и экземпляров классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7d909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from configs.config import settings\n",
    "from src.data_utils import DataUtils\n",
    "from src.next_token_dataset import NextTokenDataset, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf983f63",
   "metadata": {},
   "source": [
    "2. Загрузка и очистка входных текстовых данных.  \n",
    "Разделение данных на выборки.  \n",
    "Сохранение токенизированных текстов в csv файлы.  \n",
    "\n",
    "При повторных расчётах, если csv файлы уже были сформированы, можно перейти сразу в блок 3.2 для формирования DataLoader'ов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6798a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils = DataUtils()\n",
    "data_utils.load_and_clean('data/tweets50000.txt')\n",
    "# data_utils.load_and_clean('data/tweets.txt')\n",
    "data_utils.texts_stats()\n",
    "data_utils.save_cleaned_data(data_utils.data, 'data/cleaned_tweets.txt')\n",
    "print(data_utils.data[-1])\n",
    "\n",
    "train, val, test = data_utils.split_data(data_utils.data)\n",
    "data_utils.save_cleaned_data(train, 'data/train.txt')\n",
    "data_utils.save_cleaned_data(val, 'data/val.txt')\n",
    "data_utils.save_cleaned_data(test, 'data/test.txt')\n",
    "all_texts = NextTokenDataset(data_utils.data, tokenizer, seq_len=settings.SEQ_LEN)\n",
    "\n",
    "train_tokens, val_tokens, test_tokens = data_utils.split_data(all_texts.tokens)\n",
    "\n",
    "data_utils.save_tokenized_data_csv(train_tokens, 'data/train_tokens.csv')\n",
    "data_utils.save_tokenized_data_csv(val_tokens, 'data/val_tokens.csv')\n",
    "data_utils.save_tokenized_data_csv(test_tokens, 'data/test_tokens.csv')\n",
    "\n",
    "print(all_texts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ef58b",
   "metadata": {},
   "source": [
    "3. Формирование датасетов и даталоадеров из очищенных данных.  \n",
    "3.1 Первый блок ипользуется для формирования DataLoader'ов на основе текстов.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# на основе текстов\n",
    "# тренировочный, валидационный и тестовые датасеты из очищенных данных\n",
    "train_dataset = NextTokenDataset(train, tokenizer, seq_len=settings.SEQ_LEN)\n",
    "val_dataset = NextTokenDataset(val, tokenizer, seq_len=settings.SEQ_LEN)\n",
    "test_dataset = NextTokenDataset(test, tokenizer, seq_len=settings.SEQ_LEN)\n",
    "print(\"Datasets complete!\")\n",
    "# даталоадеры\n",
    "train_loader = DataLoader(train_dataset, batch_size=settings.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=settings.BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=settings.BATCH_SIZE)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Val dataset size: {len(val_dataset)}, Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Train loader size: {len(train_loader)}, Val loader size: {len(val_loader)}, Test loader size: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533541a4",
   "metadata": {},
   "source": [
    "3.2 Второй блок ипользуется для формирования DataLoader'ов на основе токенов, предварительно сохранённых в csv файлы.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# на основе файлов с токенами\n",
    "train = val = test = []\n",
    "# тренировочный, валидационный и тестовые датасеты из подготовленных токенов\n",
    "train_dataset = NextTokenDataset(train, tokenizer, seq_len=settings.SEQ_LEN, tokens_csv='data/train_tokens.csv')\n",
    "val_dataset = NextTokenDataset(val, tokenizer, seq_len=settings.SEQ_LEN, tokens_csv='data/val_tokens.csv')\n",
    "test_dataset = NextTokenDataset(test, tokenizer, seq_len=settings.SEQ_LEN, tokens_csv='data/test_tokens.csv')\n",
    "print(\"Datasets complete!\")\n",
    "# даталоадеры\n",
    "train_loader = DataLoader(train_dataset, batch_size=settings.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=settings.BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=settings.BATCH_SIZE)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Val dataset size: {len(val_dataset)}, Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Train loader size: {len(train_loader)}, Val loader size: {len(val_loader)}, Test loader size: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e636d7",
   "metadata": {},
   "source": [
    "4. Загрузка экземпляра модели, устройства обработки.  \n",
    "Вывод параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ecf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lstm_model import model, device, count_parameters\n",
    "\n",
    "param_count = count_parameters(model)\n",
    "print(f\"{param_count:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916640d7",
   "metadata": {},
   "source": [
    "5. Загрузка экземпляра класса оценки модели.  \n",
    "Параметр compute_rouge указывает на необходимость расчёта метрики ROUGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval_lstm import EvalLSTM\n",
    "\n",
    "evaluator = EvalLSTM(model, device, compute_rouge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdef629",
   "metadata": {},
   "source": [
    "6. Загрузка и запуск экземпляра класса обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279710ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lstm_train import LSTMTrain\n",
    "\n",
    "lstm = LSTMTrain(model, device, evaluator)\n",
    "metrics_history = lstm.model_train(train_loader, val_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb2a83",
   "metadata": {},
   "source": [
    "7. Строим графики метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "df = pd.DataFrame(metrics_history)\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(df['epoch'], df['train_loss'], label='Train Loss', color='blue', marker='o')\n",
    "ax1.plot(df['epoch'], df['val_loss'], label='Val Loss', color='red', marker='s')\n",
    "ax1.plot(df['epoch'], df['val_ppl'], label='Val PPL', color='orange', marker='v')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='black')\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df['epoch'], df['val_accuracy'], label='Val Accuracy', color='green', marker='^', linestyle='--')\n",
    "ax2.set_ylabel('Accuracy', color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "plt.title('Loss and Accuracy over Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3d2df",
   "metadata": {},
   "source": [
    "8. Загрузка предварительно обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка обученной модели\n",
    "model.load_state_dict(torch.load(f\"{settings.MODEL_PATH}_{settings.HIDDEN_DIM}_{settings.NUM_LAYERS}layer_state_dict.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4693d",
   "metadata": {},
   "source": [
    "9. Оценка работы генерации твитов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86297479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# примеры твитов из тестовой выборки\n",
    "'''\n",
    "still trying to figure out the purpose of twitter bah headed back to beautiful bryan texas\n",
    "just got done watching august rush love it sooo much\n",
    "oliyoung fair enough well ill have a crack at it over the weekend where did you get your formulae from\n",
    "cant wait to get a car walking in the cold is seriously not fun anymore\n",
    "'''\n",
    "\n",
    "prompt = \"cant wait to get a car walking in\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "text = model.generate(input_ids, n_tokens=10, tokenizer=tokenizer)\n",
    "print(f\"{prompt} {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d461c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"cant wait to get a car walking in\"\n",
    "input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "text = model.generate(input_ids, n_tokens=10, tokenizer=tokenizer, sampling=True)\n",
    "print(f\"{prompt} {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f83ea",
   "metadata": {},
   "source": [
    "10. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{settings.MODEL_PATH}_{settings.HIDDEN_DIM}_{settings.NUM_LAYERS}layer_state_dict.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc38bda",
   "metadata": {},
   "source": [
    "11. Генерация текста с помощью трансформера.  \n",
    "Модель на основе GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval_transformer_pipeline import EvalTransformer\n",
    "\n",
    "gpt_model = \"distilgpt2\"\n",
    "evaluator = EvalTransformer(model_name=gpt_model)\n",
    "\n",
    "dataset_prompts = [\n",
    "    \"still trying to figure out the purpose of twitter\",\n",
    "    \"oliyoung fair enough well ill have a crack at it\",\n",
    "    \"just got done watching august\"\n",
    "]\n",
    "\n",
    "dataset_references = [\n",
    "    \"still trying to figure out the purpose of twitter bah headed back to beautiful bryan texas\",\n",
    "    \"oliyoung fair enough well ill have a crack at it over the weekend where did you get your formulae from\",\n",
    "    \"just got done watching august rush love it sooo much\"\n",
    "]\n",
    "\n",
    "# Параметры генерации\n",
    "generation_params = {\n",
    "    \"max_length\": 20,  # промпт + ответ\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.95,\n",
    "    \"no_repeat_ngram_size\": 2\n",
    "}\n",
    "\n",
    "# Запуск валидации\n",
    "results = evaluator.validate(\n",
    "    prompts=dataset_prompts,\n",
    "    references=dataset_references,\n",
    "    generation_config=generation_params\n",
    ")\n",
    "\n",
    "# Вывод результатов\n",
    "print('\\n--- Результаты метрик ---')\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print('\\n--- Пример генерации ---')\n",
    "sample_out = evaluator.generate_texts([dataset_prompts[2]], **generation_params)\n",
    "print(f\"Промпт: {dataset_prompts[2]}\")\n",
    "print(f\"Референс: {dataset_references[2]}\")\n",
    "print(f\"Генерация: {sample_out[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
